#!python

import sys
import logging
import argparse
import random
import multiprocessing

from requests_futures.sessions import FuturesSession
from requests.exceptions import ConnectionError, TooManyRedirects
from warcio.warcwriter import WARCWriter
from warcio.statusandheaders import StatusAndHeaders

from aiu import ArchiveItCollection
from aiu import convert_LinkTimeMap_to_dict

logger = logging.getLogger(__name__)

cpu_count = multiprocessing.cpu_count()

archive_mappings = {
    "wayback.archive-it.org": ( '/http', 'id_/http' ),
    "web.archive.org": ( '/http', 'id_/http' )
}

def generate_raw_urim(urim):
    """Generates a raw URI-M based on the archive it belongs to. Supported
    URI patterns are found in `archive_mappings`.
    """

    raw_urim = urim

    for domainname in archive_mappings:

        if domainname in urim:

            search_pattern = archive_mappings[domainname][0]
            replacement_pattern = archive_mappings[domainname][1]

            # if urim is already a raw urim, do nothing
            if replacement_pattern not in urim:

                raw_urim = urim.replace(
                    search_pattern, replacement_pattern
                )

            break

    return raw_urim

def process_arguments(args):

    parser = argparse.ArgumentParser(prog="{}".format(args[0]),
        description='Produces a WARC from the seeds of an Archive-It collection.',
        formatter_class=argparse.RawTextHelpFormatter)

    parser.add_argument('-c', '--collection_id', dest='collection_id',
        required=True, help="The identifier of the Archive-It collection to download")

    parser.add_argument('-o', '--output', dest='output_filename',
        required=True, help="The filename of the WARC to which the collection seeds will be written")

    args = parser.parse_args()

    return args

def get_head_responses(session, uris):
    """This function creates a futures object for each URI-M in `uris,
    using an existing `session` object from requests-futures. Only HEAD
    requests are performed.
    """

    futures = {}

    for uri in uris:

        logger.debug("issuing HEAD on uri {}".format(uri))

        futures[uri] = session.head(uri, allow_redirects=True)

    return futures

def get_uri_responses(session, raw_uris):
    """This function creates a futures object for each URI-M in `raw_uris`,
    using an existing `session` object from requests-futures. Only GET
    requests are performed.
    """

    futures = {}

    for uri in raw_uris:

        logger.debug("issuing GET on uri {}".format(uri))

        futures[uri] = session.get(uri)

    return futures

def generate_archiveit_urits(cid, seed_uris):
    """This function generates TimeMap URIs (URI-Ts) for a list of `seed_uris`
    from an Archive-It colleciton specified by `cid`.
    """

    urit_list = []

    for urir in seed_uris:
        urit = "http://wayback.archive-it.org/{}/timemap/link/{}".format(
            cid, urir
        )

        urit_list.append(urit)  

    return urit_list

def discover_raw_urims(urimlist, futures=None):
    """This function checks that the URI-Ms in `urimlist` are valid mementos,
    following all redirects and checking for a Memento-Datetime header.
    """

    raw_urimdata = {}
    errordata = {}

    if futures == None:
        with FuturesSession(max_workers=cpu_count) as session:
            futures = get_head_responses(session, urimlist)

    working_uri_list = list(futures.keys())

    completed_urims = []

    # for urim in list_generator(working_uri_list):
    while len(completed_urims) < len(list(working_uri_list)):

        urim = random.choice(
            list(set(working_uri_list) - set(completed_urims))
        )

        logger.debug("checking if URI-M {} is done downloading".format(urim))

        if futures[urim].done():

            logger.debug("searching for raw version of URI-M {}".format(urim))

            try:

                response = futures[urim].result()

                if "memento-datetime" in response.headers:

                    if len(response.history) == 0:
                        raw_urimdata[urim] = generate_raw_urim(urim)
                    else:
                        raw_urimdata[urim] = generate_raw_urim(response.url)

                    logger.debug("added raw URI-M {} associated with URI-M {}"
                        " to the list to be downloaded".format(raw_urimdata[urim], urim))

                else:

                    warn_msg = "No Memento-Datetime in Response Headers for " \
                        "URI-M {}".format(urim)

                    logger.warning(warn_msg)
                    errordata[urim] = warn_msg

            except ConnectionError as e:
                logger.warning("While acquiring memento at {} there was an error of {}, "
                    "this event is being recorded".format(urim, repr(e)))
                errordata[urim] = repr(e)

            except TooManyRedirects as e:
                logger.warning("While acquiring memento at {} there was an error of {},"
                    "this event is being recorded".format(urim, repr(e)))
                errordata[urim] = repr(e)

            finally:
                logger.debug("Removing URI-M {} from the processing list".format(urim))
                completed_urims.append(urim)

    return raw_urimdata, errordata

def list_generator(input_list):
    """This function generates the next item in a list. It is useful for lists
    that have their items deleted while one is iterating through them.
    """

    logger.debug("list generator called")

    while len(input_list) > 0:
        for item in input_list:
            logger.debug("list now has {} items".format(len(input_list)))
            logger.debug("yielding {}".format(item))
            yield item

if __name__ == '__main__':

    args = process_arguments(sys.argv)

    timemap_data = {}

    logger = logging.getLogger()
    loglevel = logging.INFO
    logging.basicConfig( 
        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
        level=loglevel)

    logger.info("Starting Archive-It seed mementos to WARC")
    logger.info("Using collection ID {}".format(args.collection_id))
    logger.info("Data will be written out to {}".format(args.output_filename))

    # 1. get seeds for Archive-It collection
    aic = ArchiveItCollection(collection_id=args.collection_id)
    seed_uris = aic.list_seed_uris()

    # 2. get list of URI-Ts
    urit_list = generate_archiveit_urits(args.collection_id, seed_uris)

    with FuturesSession(max_workers=cpu_count) as session:
        futures = get_uri_responses(session, urit_list)

    working_uri_list = list(futures.keys())

    for urit in list_generator(working_uri_list):

        logging.debug("checking if URI-T {} is done downloading".format(urit))

        if futures[urit].done():

            logger.debug("URI-T {} is done, extracting content".format(urit))

            try:
                response = futures[urit].result()

                http_status = response.status_code

                if http_status == 200:

                    timemap_content = response.text
                    timemap_headers = dict(response.headers)
                    timemap_headers["http-status"] = http_status

                    logger.info("adding TimeMap content for URI-T {}".format(urit))
                    # logger.debug("Content: {}".format(timemap_content))
                    # logger.debug("Headers: {}".format(
                    #     pprint.pformat(timemap_headers)
                    #     ))

                    timemap_data[urit] = convert_LinkTimeMap_to_dict(timemap_content, skipErrors=True)

                # TODO: else store connection errors in CollectionModel
                working_uri_list.remove(urit)
            
            except ConnectionError:

                logger.warning("There was a connection error while attempting "
                    "to download URI-T {}".format(urit))

                # TODO: store connection errors in CollectionModel
                working_uri_list.remove(urit)

            except TooManyRedirects:

                logger.warning("There were too many redirects while attempting "
                    "to download URI-T {}".format(urit))

                # TODO: store connection errors in CollectionModel
                working_uri_list.remove(urit)

    urims = []

    for urit in timemap_data:

        timemap = timemap_data[urit]

        try:
            for memento in timemap["mementos"]["list"]:
                # raw_urim = generate_raw_urim(memento["uri"])
                # urims.append(raw_urim)
                urims.append(memento["uri"])
        except Exception as e:
            logger.error("Error encountered processing TimeMap at {}".format(urit))
            logger.error("TimeMap Object: {}".format(timemap))
            raise e

    raw_urimdata, errordata = discover_raw_urims(urims)

    invert_raw_urimdata_mapping = {}
    raw_urims = []

    for urim in raw_urimdata:
        raw_urim = raw_urimdata[urim]
        invert_raw_urimdata_mapping.setdefault(raw_urim, []).append( urim )
        raw_urims.append(raw_urim)

    logger.info("Issuing requests for {} raw mementos".format(len(raw_urims)))
    
    with FuturesSession(max_workers=cpu_count) as session:
        futures = get_uri_responses(session, raw_urims)

    with open(args.output_filename, 'wb') as output:
        writer = WARCWriter(output, gzip=True)

        completed_raw_urims = []
        leftovers = list(set(raw_urims) - set(completed_raw_urims))

        while len(leftovers) > 0:

            raw_urim = random.choice(leftovers)

            logger.info("Processing raw URI-M {} associated with URI-M {}".format(
                raw_urim, invert_raw_urimdata_mapping[raw_urim]))

            if futures[raw_urim].done():

                logger.info("Raw URI-M {} is done".format(raw_urim))

                response = futures[raw_urim].result()

                http_status = response.status_code
                # memento_content = bytes(response.text, 'utf8')
                memento_content = response.raw
                memento_headers = dict(response.headers)    
                memento_headers["http-status"] = http_status

                # sometimes, via redirects, the different URI-Ms end up at the 
                # same raw URI-M
                logger.info("There are {} URI-Ms leading to raw URI-M {}".format(
                    len(invert_raw_urimdata_mapping[raw_urim]), raw_urim
                ))

                http_headers = StatusAndHeaders(
                    memento_headers["http-status"], memento_headers.items(), 
                    protocol="HTTP/1.1")

                for urim in invert_raw_urimdata_mapping[raw_urim]:
                    record = writer.create_warc_record(urim, 'response', 
                        payload=memento_content, http_headers=http_headers)

                    writer.write_record(record)

                    # collectionmodel.addMemento(urim, memento_content, memento_headers)

                logger.debug("Removing raw URI-M {} from processing list".format(raw_urim))

                completed_raw_urims.append(raw_urim)

            leftovers = list(set(raw_urims) - set(completed_raw_urims))

        # TODO: what to do with errordata?

    logger.info("Finished run")